{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "581f0fe3",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e477c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/18 14:37:50 WARN Utils: Your hostname, xuzhengs-MacBook-Pro-2.local, resolves to a loopback address: 127.0.0.1; using 10.5.82.157 instead (on interface en0)\n",
      "25/09/18 14:37:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/18 14:37:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PySpark\n",
    "import shutil, os, glob, re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, to_date, to_timestamp, when, expr, date_trunc, count, avg, coalesce\n",
    "from pyspark.sql.functions import explode, split, trim\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import count as Fcount\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, trim, upper, initcap\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BNPL_Part1_Data\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626af963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing CSV\n",
    "consumer_fraud = spark.read.csv(\"part1_data/consumer_fraud_probability.csv\", \n",
    "                                header=True, inferSchema=True)\n",
    "\n",
    "merchant_fraud = spark.read.csv(\"part1_data/merchant_fraud_probability.csv\", \n",
    "                                header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "\n",
    "# Importing Parquet \n",
    "consumer_details = spark.read.parquet(\"part1_data/consumer_user_details.parquet\")\n",
    "tbl_merchants = spark.read.parquet(\"part1_data/tbl_merchants.parquet\")\n",
    "\n",
    "# Importing Shpile\n",
    "SA2_PATH = \"external_data/zone/AU_zone.shp\"   # SA2 Shpile File (Map)\n",
    "MIN_SAMPLES  = 5   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3402385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------------+-----+--------+------+-----------+\n",
      "|name             |address                      |state|postcode|gender|consumer_id|\n",
      "+-----------------+-----------------------------+-----+--------+------+-----------+\n",
      "|Yolanda Williams |413 Haney Gardens Apt. 742   |WA   |6935    |Female|1195503    |\n",
      "|Mary Smith       |3764 Amber Oval              |NSW  |2782    |Female|179208     |\n",
      "|Jill Jones MD    |40693 Henry Greens           |NT   |862     |Female|1194530    |\n",
      "|Lindsay Jimenez  |00653 Davenport Crossroad    |NSW  |2780    |Female|154128     |\n",
      "|Rebecca Blanchard|9271 Michael Manors Suite 651|WA   |6355    |Female|712975     |\n",
      "+-----------------+-----------------------------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postcode: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- consumer_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading tbl_consumer\n",
    "df = (spark.read\n",
    "        .option(\"sep\", \"|\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .csv(\"part1_data/tbl_consumer.csv\"))\n",
    "\n",
    "# Standardization of field content\n",
    "df_std = (df\n",
    "    .withColumn(\"name\", trim(col(\"name\")))\n",
    "    .withColumn(\"address\", trim(col(\"address\")))\n",
    "    .withColumn(\"state\", upper(trim(col(\"state\"))))     # Uniform capitalization\n",
    "    .withColumn(\"gender\", initcap(trim(col(\"gender\"))))  \n",
    ")\n",
    "\n",
    "# Safely write standardized data\n",
    "tmp_dir = \"part1_data/__tmp_new_tbl_consumer\"\n",
    "out_csv = \"part1_data/new_tbl_consumer.csv\"\n",
    "\n",
    "(df_std.coalesce(1)\n",
    "   .write.mode(\"overwrite\")\n",
    "   .option(\"header\", True)\n",
    "   .option(\"encoding\", \"UTF-8\")\n",
    "   .csv(tmp_dir))\n",
    "\n",
    "# Rename the output file\n",
    "import os, glob, shutil\n",
    "part = glob.glob(os.path.join(tmp_dir, \"part-*.csv\"))[0]\n",
    "shutil.move(part, out_csv)\n",
    "shutil.rmtree(tmp_dir)\n",
    "\n",
    "# Reload the standardized CSV\n",
    "new_tbl_consumer = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)      # 告诉 Spark 第一行是表头\n",
    "         .option(\"sep\", \",\")          # 你在 part1 用 Spark 写出的 CSV 默认是逗号分隔\n",
    "         .option(\"inferSchema\", True) # 自动推断类型，方便后续 join\n",
    "         .csv(out_csv)\n",
    ")\n",
    "new_tbl_consumer.show(5, truncate=False)\n",
    "new_tbl_consumer.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536b00e",
   "metadata": {},
   "source": [
    "# Data Check - Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dfbf43",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01cfff34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------------+\n",
      "|user_id|order_datetime|fraud_probability|\n",
      "+-------+--------------+-----------------+\n",
      "|   6228|    2021-12-19| 97.6298077657765|\n",
      "|  21419|    2021-12-10|99.24738020302328|\n",
      "|   5606|    2021-10-17|84.05825045251777|\n",
      "|   3101|    2021-04-17|91.42192091901347|\n",
      "|  22239|    2021-10-19|94.70342477508035|\n",
      "|  16556|    2022-02-20|89.65663294494827|\n",
      "|  10278|    2021-09-28|83.59136689427714|\n",
      "|  15790|    2021-12-30|71.77065889280253|\n",
      "|   5233|    2021-08-29|85.87123303878818|\n",
      "|    230|    2021-08-28|86.28328808934151|\n",
      "+-------+--------------+-----------------+\n",
      "only showing top 10 rows\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- fraud_probability: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the initial data\n",
    "# consumer_fraud: The fraud probability of a certain user placing an order at a certain time\n",
    "consumer_fraud.show(10)\n",
    "consumer_fraud.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78f9760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------------+\n",
      "|merchant_abn|order_datetime| fraud_probability|\n",
      "+------------+--------------+------------------+\n",
      "| 19492220327|    2021-11-28|44.403658647495355|\n",
      "| 31334588839|    2021-10-02| 42.75530083865367|\n",
      "| 19492220327|    2021-12-22|38.867790051131095|\n",
      "| 82999039227|    2021-12-19|  94.1347004808891|\n",
      "| 90918180829|    2021-09-02| 43.32551731714902|\n",
      "+------------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- merchant_abn: long (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- fraud_probability: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the initial data\n",
    "# The fraud probability of a certain merchant's order\n",
    "merchant_fraud.show(5)\n",
    "merchant_fraud.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc3c0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_fraud.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dac834b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-----+--------+------+-----------+\n",
      "|             name|             address|state|postcode|gender|consumer_id|\n",
      "+-----------------+--------------------+-----+--------+------+-----------+\n",
      "| Yolanda Williams|413 Haney Gardens...|   WA|    6935|Female|    1195503|\n",
      "|       Mary Smith|     3764 Amber Oval|  NSW|    2782|Female|     179208|\n",
      "|    Jill Jones MD|  40693 Henry Greens|   NT|     862|Female|    1194530|\n",
      "|  Lindsay Jimenez|00653 Davenport C...|  NSW|    2780|Female|     154128|\n",
      "|Rebecca Blanchard|9271 Michael Mano...|   WA|    6355|Female|     712975|\n",
      "|    Karen Chapman|2706 Stewart Oval...|  NSW|    2033|Female|     407340|\n",
      "|     Andrea Jones|   122 Brandon Cliff|  QLD|    4606|Female|     511685|\n",
      "| Stephen Williams|6804 Wright Crest...|   WA|    6056|  Male|     448088|\n",
      "|  Stephanie Reyes|5813 Denise Land ...|  NSW|    2482|Female|     650435|\n",
      "| Jillian Gonzales|461 Ryan Common S...|  VIC|    3220|Female|    1058499|\n",
      "+-----------------+--------------------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postcode: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- consumer_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the initial data\n",
    "# Basic demographic information of consumers\n",
    "new_tbl_consumer.show(10)\n",
    "new_tbl_consumer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b857ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|user_id|consumer_id|\n",
      "+-------+-----------+\n",
      "|      1|    1195503|\n",
      "|      2|     179208|\n",
      "|      3|    1194530|\n",
      "|      4|     154128|\n",
      "|      5|     712975|\n",
      "+-------+-----------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- consumer_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the initial data\n",
    "# Use to convert and connect by user_id and consumer_id\n",
    "consumer_details.show(5)\n",
    "consumer_details.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5916cf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+\n",
      "|                name|                tags|merchant_abn|\n",
      "+--------------------+--------------------+------------+\n",
      "|       Felis Limited|((furniture, home...| 10023283211|\n",
      "|Arcu Ac Orci Corp...|([cable, satellit...| 10142254217|\n",
      "|    Nunc Sed Company|([jewelry, watch,...| 10165489824|\n",
      "|Ultricies Digniss...|([wAtch, clock, a...| 10187291046|\n",
      "| Enim Condimentum PC|([music shops - m...| 10192359162|\n",
      "|       Fusce Company|[(gift, card, nov...| 10206519221|\n",
      "|Aliquam Enim Inco...|[(computers, comP...| 10255988167|\n",
      "|    Ipsum Primis Ltd|[[watch, clock, a...| 10264435225|\n",
      "|Pede Ultrices Ind...|([computer progra...| 10279061213|\n",
      "|           Nunc Inc.|[(furniture, home...| 10323485998|\n",
      "+--------------------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- merchant_abn: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the initial data\n",
    "# The basic information of the merchant, including industry tags\n",
    "tbl_merchants.show(10)\n",
    "tbl_merchants.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b2802",
   "metadata": {},
   "source": [
    "### Missing Values & Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "119faf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'consumer_fraud': {'sum(user_id)': 0,\n",
       "  'sum(order_datetime)': 0,\n",
       "  'sum(fraud_probability)': 0},\n",
       " 'merchant_fraud': {'sum(merchant_abn)': 0,\n",
       "  'sum(order_datetime)': 0,\n",
       "  'sum(fraud_probability)': 0},\n",
       " 'tbl_consumer': {'sum(name)': 0,\n",
       "  'sum(address)': 0,\n",
       "  'sum(state)': 0,\n",
       "  'sum(postcode)': 0,\n",
       "  'sum(gender)': 0,\n",
       "  'sum(consumer_id)': 0},\n",
       " 'consumer_details': {'sum(user_id)': 0, 'sum(consumer_id)': 0},\n",
       " 'tbl_merchants': {'sum(name)': 0, 'sum(tags)': 0, 'sum(merchant_abn)': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Missing Values\n",
    "results = {}\n",
    "for df, name in [(consumer_fraud, \"consumer_fraud\"),\n",
    "                 (merchant_fraud, \"merchant_fraud\"),\n",
    "                 (new_tbl_consumer, \"tbl_consumer\"),\n",
    "                 (consumer_details, \"consumer_details\"),\n",
    "                 (tbl_merchants, \"tbl_merchants\")]:\n",
    "    res = df.select([col(c).isNull().cast(\"int\").alias(c) for c in df.columns])\\\n",
    "            .groupBy().sum().collect()\n",
    "    results[name] = res[0].asDict()\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ef1fa76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|distinct_consumer_id|\n",
      "+--------------------+\n",
      "|              499999|\n",
      "+--------------------+\n",
      "\n",
      "tbl_consumer total rows: 499999\n",
      "+---------------------+\n",
      "|distinct_merchant_abn|\n",
      "+---------------------+\n",
      "|                 4026|\n",
      "+---------------------+\n",
      "\n",
      "tbl_merchants total rows: 4026\n",
      "+----------------+\n",
      "|distinct_user_id|\n",
      "+----------------+\n",
      "|          499999|\n",
      "+----------------+\n",
      "\n",
      "+--------------------+\n",
      "|distinct_consumer_id|\n",
      "+--------------------+\n",
      "|              499999|\n",
      "+--------------------+\n",
      "\n",
      "consumer_details total rows: 499999\n",
      "+-------+-----+\n",
      "|user_id|count|\n",
      "+-------+-----+\n",
      "|     26|    1|\n",
      "|    321|    1|\n",
      "|     41|    1|\n",
      "|    327|    1|\n",
      "|     29|    1|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "+-----------+-----+\n",
      "|consumer_id|count|\n",
      "+-----------+-----+\n",
      "|     367113|    1|\n",
      "|     845767|    1|\n",
      "|    1174371|    1|\n",
      "|      44111|    1|\n",
      "|    1255411|    1|\n",
      "+-----------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "### Check the uniqueness of the primary key\n",
    "\n",
    "# tbl_consumer check\n",
    "new_tbl_consumer.select(countDistinct(\"consumer_id\").alias(\"distinct_consumer_id\")).show()\n",
    "print(\"tbl_consumer total rows:\", new_tbl_consumer.count())\n",
    "\n",
    "# tbl_merchants check\n",
    "tbl_merchants.select(countDistinct(\"merchant_abn\").alias(\"distinct_merchant_abn\")).show()\n",
    "print(\"tbl_merchants total rows:\", tbl_merchants.count())\n",
    "\n",
    "# consumer_user_details check\n",
    "consumer_details.select(countDistinct(\"user_id\").alias(\"distinct_user_id\")).show()\n",
    "consumer_details.select(countDistinct(\"consumer_id\").alias(\"distinct_consumer_id\")).show()\n",
    "print(\"consumer_details total rows:\", consumer_details.count())\n",
    "\n",
    "# user_id check\n",
    "consumer_details.groupBy(\"user_id\").count().orderBy(\"count\", ascending=False).show(5)\n",
    "\n",
    "# consumer_id check\n",
    "consumer_details.groupBy(\"consumer_id\").count().orderBy(\"count\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e34e2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consumer_fraud low<0: 0 high>100: 0\n",
      "merchant_fraud low<0: 0 high>100: 0\n"
     ]
    }
   ],
   "source": [
    "### Field type and value range correction\n",
    "\n",
    "# Uniform time type\n",
    "consumer_fraud  = consumer_fraud.withColumn(\"order_date\", to_date(col(\"order_datetime\")))\n",
    "merchant_fraud  = merchant_fraud.withColumn(\"order_date\", to_date(col(\"order_datetime\")))\n",
    "\n",
    "# fraud_probability range limit: [0, 100]\n",
    "for df_name, df in [(\"consumer_fraud\", consumer_fraud), (\"merchant_fraud\", merchant_fraud)]:\n",
    "    bad_low  = df.filter(col(\"fraud_probability\") < 0).count()\n",
    "    bad_high = df.filter(col(\"fraud_probability\") > 100).count()\n",
    "    print(df_name, \"low<0:\", bad_low, \"high>100:\", bad_high)\n",
    "\n",
    "# Create an \"Exception Label\" field for both consumer and merchant\n",
    "consumer_fraud = consumer_fraud.withColumn(\n",
    "    \"is_prob_out_of_range\",\n",
    "    when((col(\"fraud_probability\") < 0) | (col(\"fraud_probability\") > 100), 1).otherwise(0)\n",
    ")\n",
    "merchant_fraud = merchant_fraud.withColumn(\n",
    "    \"is_prob_out_of_range\",\n",
    "    when((col(\"fraud_probability\") < 0) | (col(\"fraud_probability\") > 100), 1).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd609ee2",
   "metadata": {},
   "source": [
    "### Distribution Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bf5fdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== consumer_fraud ==\n",
      "+-----------------+----------------+----------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|              min|             p05|             p25|            median|              mean|               p75|               p95|              max|\n",
      "+-----------------+----------------+----------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|8.287143531552802|8.51799998867597|9.63404401745533|11.735485430281175|15.120090644154585|16.213757783429045|34.346776609318646|99.24738020302328|\n",
      "+-----------------+----------------+----------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n",
      "== merchant_fraud ==\n",
      "+-----------------+-----------------+-----------------+----------------+------------------+------------------+-----------------+----------------+\n",
      "|              min|              p05|              p25|          median|              mean|               p75|              p95|             max|\n",
      "+-----------------+-----------------+-----------------+----------------+------------------+------------------+-----------------+----------------+\n",
      "|18.21089142894488|24.73500985964068|28.98838429202166|32.3890854385705|40.419334695018094|48.642698518841016|72.73069736562613|94.1347004808891|\n",
      "+-----------------+-----------------+-----------------+----------------+------------------+------------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Show Fraud Probability\n",
    "\n",
    "def prob_summary(df, colname=\"fraud_probability\"):\n",
    "    return df.selectExpr(\n",
    "        f\"min({colname}) as min\",\n",
    "        f\"percentile_approx({colname}, 0.05) as p05\",\n",
    "        f\"percentile_approx({colname}, 0.25) as p25\",\n",
    "        f\"percentile_approx({colname}, 0.50) as median\",\n",
    "        f\"avg({colname}) as mean\",\n",
    "        f\"percentile_approx({colname}, 0.75) as p75\",\n",
    "        f\"percentile_approx({colname}, 0.95) as p95\",\n",
    "        f\"max({colname}) as max\"\n",
    "    )\n",
    "\n",
    "print(\"== consumer_fraud ==\"); prob_summary(consumer_fraud).show()\n",
    "print(\"== merchant_fraud ==\"); prob_summary(merchant_fraud).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b843186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer rows: 34864, Merchant rows: 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1100x380 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Fraud Probability Visualisation\n",
    "\n",
    "# Extract All data in the form of Pandas\n",
    "cf_pd = (consumer_fraud\n",
    "         .select(col(\"fraud_probability\").cast(\"double\").alias(\"fraud_probability\"))\n",
    "         .toPandas())\n",
    "mf_pd = (merchant_fraud\n",
    "         .select(col(\"fraud_probability\").cast(\"double\").alias(\"fraud_probability\"))\n",
    "         .toPandas())\n",
    "\n",
    "# Basic cleaning: Remove possible NaN/ illegal values and uniformly crop them to [0,100]\n",
    "cf_pd = cf_pd.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "mf_pd = mf_pd.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "cf_pd[\"fraud_probability\"] = cf_pd[\"fraud_probability\"].clip(0, 100)\n",
    "mf_pd[\"fraud_probability\"] = mf_pd[\"fraud_probability\"].clip(0, 100)\n",
    "\n",
    "print(f\"Consumer rows: {len(cf_pd)}, Merchant rows: {len(mf_pd)}\")\n",
    "\n",
    "# Unify the plotting style\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "x_min, x_max = 0, 100\n",
    "\n",
    "# KDE Density Plot \n",
    "plt.figure(figsize=(11,6))\n",
    "\n",
    "# Plot the Consumer fraud rate\n",
    "sns.kdeplot(\n",
    "    data=cf_pd,\n",
    "    x=\"fraud_probability\",\n",
    "    fill=True, common_norm=False,\n",
    "    color=\"#4063D8\", alpha=0.35,\n",
    "    bw_adjust=1.0,                      \n",
    "    label=f\"Consumer (n={len(cf_pd)})\"\n",
    ")\n",
    "\n",
    "# Plot the Merchant fraud rate\n",
    "sns.kdeplot(\n",
    "    data=mf_pd,\n",
    "    x=\"fraud_probability\",\n",
    "    fill=True, common_norm=False,\n",
    "    color=\"#E36F47\", alpha=0.35,\n",
    "    bw_adjust=1.3,\n",
    "    label=f\"Merchant (n={len(mf_pd)})\"\n",
    ")\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.xlabel(\"Fraud Probability\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Fraud Probability — Density Plot (Consumer vs Merchant)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plot (View median and tail)\n",
    "plt.figure(figsize=(11,3.8))\n",
    "sns.boxplot(\n",
    "    data=pd.DataFrame({\n",
    "        \"Consumer\": cf_pd[\"fraud_probability\"],\n",
    "        \"Merchant\": mf_pd[\"fraud_probability\"]\n",
    "    }),\n",
    "    orient=\"h\",\n",
    "    palette=[\"#4063D8\", \"#E36F47\"],\n",
    "    width=0.35\n",
    ")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.xlabel(\"Fraud Probability\")\n",
    "plt.title(\"Fraud Probability — Boxplot (Consumer vs Merchant)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35bca331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------------+\n",
      "|month              |n_txn|avg_prob          |\n",
      "+-------------------+-----+------------------+\n",
      "|2021-02-01 00:00:00|18   |15.891153513870623|\n",
      "|2021-03-01 00:00:00|539  |14.686233949236382|\n",
      "|2021-04-01 00:00:00|571  |14.605609077107525|\n",
      "|2021-05-01 00:00:00|666  |14.842259160438802|\n",
      "|2021-06-01 00:00:00|655  |14.228753068906279|\n",
      "|2021-07-01 00:00:00|750  |14.725419354515335|\n",
      "|2021-08-01 00:00:00|1256 |15.271882727231446|\n",
      "|2021-09-01 00:00:00|4520 |15.120578016391924|\n",
      "|2021-10-01 00:00:00|4917 |15.30227000413199 |\n",
      "|2021-11-01 00:00:00|7133 |15.046375701890268|\n",
      "|2021-12-01 00:00:00|6861 |15.026077263616688|\n",
      "|2022-01-01 00:00:00|3670 |15.349828952019694|\n",
      "+-------------------+-----+------------------+\n",
      "only showing top 12 rows\n",
      "+-------------------+-----+------------------+\n",
      "|month              |n_txn|avg_prob          |\n",
      "+-------------------+-----+------------------+\n",
      "|2021-03-01 00:00:00|1    |69.08556869902988 |\n",
      "|2021-04-01 00:00:00|1    |32.99497823665355 |\n",
      "|2021-08-01 00:00:00|2    |58.10705076146472 |\n",
      "|2021-09-01 00:00:00|10   |54.35126741245779 |\n",
      "|2021-10-01 00:00:00|10   |37.243835357674314|\n",
      "|2021-11-01 00:00:00|51   |35.967338098345124|\n",
      "|2021-12-01 00:00:00|25   |42.38269546956082 |\n",
      "|2022-01-01 00:00:00|6    |41.20731497206575 |\n",
      "|2022-02-01 00:00:00|8    |41.55161993563929 |\n",
      "+-------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Look at the magnitude and average risk by time (month)\n",
    "\n",
    "cf_month = (consumer_fraud\n",
    "            .withColumn(\"month\", date_trunc(\"month\", col(\"order_date\")))\n",
    "            .groupBy(\"month\")\n",
    "            .agg(count(\"*\").alias(\"n_txn\"),\n",
    "                 avg(\"fraud_probability\").alias(\"avg_prob\"))\n",
    "            .orderBy(\"month\"))\n",
    "cf_month.show(12, truncate=False)\n",
    "\n",
    "mf_month = (merchant_fraud\n",
    "            .withColumn(\"month\", date_trunc(\"month\", col(\"order_date\")))\n",
    "            .groupBy(\"month\")\n",
    "            .agg(count(\"*\").alias(\"n_txn\"),\n",
    "                 avg(\"fraud_probability\").alias(\"avg_prob\"))\n",
    "            .orderBy(\"month\"))\n",
    "mf_month.show(12, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b37e4fe",
   "metadata": {},
   "source": [
    "### Correlation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7802c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----+\n",
      "|state|avg_prob          |n    |\n",
      "+-----+------------------+-----+\n",
      "|TAS  |15.384440242109859|1332 |\n",
      "|NT   |15.308708799890276|480  |\n",
      "|QLD  |15.27782060075581 |5109 |\n",
      "|VIC  |15.162124631050196|8129 |\n",
      "|SA   |15.144591238945795|3926 |\n",
      "|NSW  |15.13376173146009 |10037|\n",
      "|WA   |14.831025833813962|5538 |\n",
      "|ACT  |14.408422510779635|313  |\n",
      "+-----+------------------+-----+\n",
      "\n",
      "+-----------+------------------+-----+\n",
      "|gender     |avg_prob          |n    |\n",
      "+-----------+------------------+-----+\n",
      "|Undisclosed|15.27125840758897 |3580 |\n",
      "|Male       |15.210206353750316|15739|\n",
      "|Female     |14.994036495140978|15545|\n",
      "+-----------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Connect consumer profiles such as state, gender and risk\n",
    "\n",
    "# user_id -> consumer_id -> tbl_consumer\n",
    "cf_joined = (consumer_fraud\n",
    "             .join(consumer_details, \"user_id\", \"left\")\n",
    "             .join(new_tbl_consumer, \"consumer_id\", \"left\"))\n",
    "\n",
    "# The average fraud probability by state/gender and the sample size\n",
    "cf_by_state  = cf_joined.groupBy(\"state\").agg(avg(\"fraud_probability\").alias(\"avg_prob\"),\n",
    "                                              count(\"*\").alias(\"n\")).orderBy(col(\"avg_prob\").desc())\n",
    "cf_by_gender = cf_joined.groupBy(\"gender\").agg(avg(\"fraud_probability\").alias(\"avg_prob\"),\n",
    "                                               count(\"*\").alias(\"n\")).orderBy(col(\"avg_prob\").desc())\n",
    "\n",
    "cf_by_state.show(truncate=False)\n",
    "cf_by_gender.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "158d71d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Visualize fraud probability and other correlations\n",
    "\n",
    "# Switch to Pandas\n",
    "cf_by_state_pd = cf_by_state.toPandas()\n",
    "cf_by_gender_pd = cf_by_gender.toPandas()\n",
    "\n",
    "# Set a unified style\n",
    "sns.set(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Average Fraud Probability by State\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    data=cf_by_state_pd.sort_values(\"avg_prob\", ascending=False),\n",
    "    x=\"avg_prob\", y=\"state\",\n",
    "    palette=\"Blues_d\"\n",
    ")\n",
    "plt.xlabel(\"Average Fraud Probability\")\n",
    "plt.ylabel(\"State\")\n",
    "plt.title(\"Average Fraud Probability by State (Consumer)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Average Fraud Probability by Gender\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.barplot(\n",
    "    data=cf_by_gender_pd.sort_values(\"avg_prob\", ascending=False),\n",
    "    x=\"gender\", y=\"avg_prob\",\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.ylabel(\"Average Fraud Probability\")\n",
    "plt.title(\"Average Fraud Probability by Gender (Consumer)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ae01d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-----+\n",
      "|gender_norm|n     |pct  |\n",
      "+-----------+------+-----+\n",
      "|Male       |224979|45.0 |\n",
      "|Female     |224946|44.99|\n",
      "|Undisclosed|50074 |10.01|\n",
      "+-----------+------+-----+\n",
      "\n",
      "Total rows: 499999\n"
     ]
    }
   ],
   "source": [
    "### Check the weight of the gender category\n",
    "\n",
    "# Unify the gender writing style for statistical purposes only\n",
    "gender_stat = (\n",
    "    new_tbl_consumer\n",
    "    .withColumn(\n",
    "        \"gender_norm\",\n",
    "        F.when(F.lower(F.col(\"gender\")).isin(\"male\",\"m\"), \"Male\")\n",
    "         .when(F.lower(F.col(\"gender\")).isin(\"female\",\"f\"), \"Female\")\n",
    "         .otherwise(\"Undisclosed\")    # Including null values/unknown/others\n",
    "    )\n",
    "    .groupBy(\"gender_norm\")\n",
    "    .agg(F.count(\"*\").alias(\"n\"))\n",
    ")\n",
    "\n",
    "total_n = gender_stat.agg(F.sum(\"n\")).first()[0]\n",
    "\n",
    "gender_stat = (\n",
    "    gender_stat\n",
    "    .withColumn(\"pct\", F.round(F.col(\"n\")/F.lit(total_n)*100, 2))\n",
    "    .orderBy(F.desc(\"n\"))\n",
    ")\n",
    "\n",
    "gender_stat.show(truncate=False)\n",
    "print(f\"Total rows: {total_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bfc20ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringType()\n",
      "+------------------------------------+---+------------------+\n",
      "|tag                                 |n  |avg_prob          |\n",
      "+------------------------------------+---+------------------+\n",
      "|NULL                                |19 |44.929074633541305|\n",
      "|home furnishings and equipment shops|5  |44.62062407904109 |\n",
      "|furniture                           |5  |44.62062407904109 |\n",
      "|except appliances                   |5  |44.62062407904109 |\n",
      "|and manufacturers                   |5  |44.62062407904109 |\n",
      "|a                                   |18 |41.2530652860964  |\n",
      "|c                                   |20 |40.159028844542576|\n",
      "|watch                               |23 |39.730783915430486|\n",
      "|clock                               |23 |39.730783915430486|\n",
      "|jewelry                             |22 |38.65594029125101 |\n",
      "|and silverware shops                |22 |38.65594029125101 |\n",
      "|antique shops - sales               |26 |38.01143524197595 |\n",
      "|and restoration services            |26 |38.01143524197595 |\n",
      "|repairs                             |26 |38.01143524197595 |\n",
      "|b                                   |55 |37.744424077694575|\n",
      "|take rate: 2.62                     |7  |35.30195968212661 |\n",
      "|take rate: 4.22                     |12 |32.36111871776698 |\n",
      "|take rate: 3.15                     |5  |32.26167451359614 |\n",
      "|telecom                             |7  |32.03607228714286 |\n",
      "|take rate: 4.93                     |8  |31.958306675667547|\n",
      "|take rate: 4.82                     |6  |31.93490297074105 |\n",
      "+------------------------------------+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Connect merchant Profile -> Industry Tags and Risks\n",
    "\n",
    "# Check the type of tags \n",
    "print(tbl_merchants.schema[\"tags\"].dataType)\n",
    "\n",
    "# Uniformly expand tags\n",
    "if isinstance(tbl_merchants.schema[\"tags\"].dataType, ArrayType):\n",
    "    # if tag is array: explore\n",
    "    merchants_exploded = (\n",
    "        tbl_merchants\n",
    "        .select(\"merchant_abn\", F.explode_outer(\"tags\").alias(\"tag_raw\"))\n",
    "    )\n",
    "else:\n",
    "    # if tag is string: clean + split + explode\n",
    "    merchants_exploded = (\n",
    "        tbl_merchants\n",
    "        .withColumn(\"tags_str\", F.coalesce(F.col(\"tags\").cast(\"string\"), F.lit(\"\")))\n",
    "        # Remove the common parenthesis symbols\n",
    "        .withColumn(\"tags_clean\", F.regexp_replace(\"tags_str\", r\"[\\[\\]\\(\\)]\", \"\"))\n",
    "        # Unify common delimiters as commas (retain/expand as needed)\n",
    "        .withColumn(\"tags_clean\", F.regexp_replace(\"tags_clean\", r\"[;/]+\", \",\"))\n",
    "        # Divide by commas and expand\n",
    "        .withColumn(\"tag_raw\", F.explode_outer(F.split(F.col(\"tags_clean\"), r\"\\s*,\\s*\")))\n",
    "    )\n",
    "\n",
    "# Standardized tags: Remove Spaces, convert to lowercase, remove empty strings\n",
    "merchants_exploded = (\n",
    "    merchants_exploded\n",
    "    .withColumn(\"tag\", F.trim(F.lower(F.col(\"tag_raw\"))))\n",
    "    .filter(F.col(\"tag\").isNotNull() & (F.col(\"tag\") != \"\"))\n",
    "    .select(\"merchant_abn\", \"tag\")\n",
    ")\n",
    "\n",
    "# The probability of fraud by associated merchants\n",
    "mf_joined = merchant_fraud.join(merchants_exploded, \"merchant_abn\", \"left\")\n",
    "\n",
    "# Calculate the sample size and average fraud probability of each tag\n",
    "mf_by_tag = (mf_joined\n",
    "             .groupBy(\"tag\")\n",
    "             .agg(F.count(\"*\").alias(\"n\"),\n",
    "                  F.avg(\"fraud_probability\").alias(\"avg_prob\"))\n",
    "             .filter(F.col(\"n\") >= 5)          \n",
    "             .orderBy(F.col(\"avg_prob\").desc()))\n",
    "\n",
    "mf_by_tag.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfca8c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+-------------------------------------------------------------------------------------------------------------------+---+\n",
      "|name                          |merchant_abn|tags                                                                                                               |len|\n",
      "+------------------------------+------------+-------------------------------------------------------------------------------------------------------------------+---+\n",
      "|Aliquam Arcu Industries       |71787414647 |((furniture, home FurnishIngs and  equipment shops,  and manufacturers, except appliances), (a), (take rate: 6.05))|115|\n",
      "|Congue Corporation            |15612785317 |[(furniture, home furnishinGs and equiPment shops, and  manufacturers, except appliances), (a), (take rate: 6.99)] |114|\n",
      "|Sed Leo Cras Industries       |65175701530 |[[furniture, home furnishings and equipment  shops, and manufacturers, except appliances], [a], [take rate: 5.64]] |114|\n",
      "|Est Vitae Ltd                 |18600250054 |((furniture, home furnishings and equipment shops, and manufacturers, except  appliances), (a), (take rate: 5.85)) |114|\n",
      "|Egestas Blandit Nam LLP       |33587480673 |((furniture, home furnishings and equipment shops, and  manufacturers, except appliances), (b), (take rate: 3.26)) |114|\n",
      "|Elit Elit Fermentum Foundation|38523766800 |((furniture, home furnishings and equipment shops,  aNd manufacturerS, except appliances), (a), (take rate: 5.94)) |114|\n",
      "|Dolor Donec Ltd               |41027080823 |((furniture, home furnishings and equipment shops,  and manufacturers, except appliances), (a), (take rate: 6.89)) |114|\n",
      "|Nibh PC                       |43152535761 |((furniture, home furnishings and  equipment shops, and manufacturers, except appliances), (b), (take rate: 3.17)) |114|\n",
      "|Mauris Nulla Integer LLC      |50551188214 |([furniture, home furnishings and equipment shops,  and manufacturers, except appliances], [c], [take rate: 1.53]) |114|\n",
      "|Eget Nisi Dictum Consulting   |52689558435 |((furnituRe, home furnishings  and equipment shops, and manufacturers, except appliances), (b), (take rate: 4.96)) |114|\n",
      "+------------------------------+------------+-------------------------------------------------------------------------------------------------------------------+---+\n",
      "only showing top 10 rows\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|tags                                                                                                               |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|((furniture, home FurnishIngs and  equipment shops,  and manufacturers, except appliances), (a), (take rate: 6.05))|\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers,  except aPpliances), (b), (take rate: 4.65)) |\n",
      "|[[furniture, home furnishings  and equipment shops, and manufacturers, except appliances], [c], [take rate: 2.72]] |\n",
      "|([furniture, home furnishings And equipment  shops, and manufacturers, except appliances], [c], [take rate: 2.67]) |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers, except  appliances), (a), (take rate: 5.85)) |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturErs, except  appliances), (a), (take rate: 5.64)) |\n",
      "|((furniture, home furnishings and equipment shops,  and manufacturers, except appliances), (a), (take rate: 6.89)) |\n",
      "|((furniture, home furnishings and  equipment shops, and manufacturers, except appliances), (b), (take rate: 3.17)) |\n",
      "|((furnituRe, home furnishings  and equipment shops, and manufacturers, except appliances), (b), (take rate: 4.96)) |\n",
      "|((furniture, home furnishings and equipment shops,  aNd manufacturerS, except appliances), (a), (take rate: 5.94)) |\n",
      "|([furniture, home furnishings and equipment shops,  and manufacturers, except appliances], [c], [take rate: 1.53]) |\n",
      "|[[furniture, home furnishings and equipment  shops, and manufacturers, except appliances], [a], [take rate: 5.64]] |\n",
      "|[[furniture, home furnishings and equipment shOps, and manufacturers,  except appliances], [a], [take rate: 5.81]] |\n",
      "|((furniture, home furnishings and equipment shops, and  manufacturers, except appliances), (b), (take rate: 3.26)) |\n",
      "|[[furniture,  home furnishings and equipment shops, and manufacturers, except appliances], [a], [take rate: 6.30]] |\n",
      "|[(furniture, home furnishinGs and equiPment shops, and  manufacturers, except appliances), (a), (take rate: 6.99)] |\n",
      "|[(furniture, home furnishings and equipment shops, and manufacturers, except applianCes), (b), (take rate: 4.09)]  |\n",
      "|((furNiture, home furnishings and equipment shops, and manufActurers, except appliances), (a), (take rate: 5.50))  |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (a), (take rate: 6.38))  |\n",
      "|([furniture, home furnishings and Equipment shops, and manufacturers, except appliances], [b], [take rate: 3.86])  |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (a), (take rate: 6.73))  |\n",
      "|[(furniture, home furnishings and equipment shops, and manufacturers, except appliances), (a), (take rate: 5.77)]  |\n",
      "|([furniture, home furnishings and equipment shops, and manufacTurers, except appliances], [c], [take rate: 2.23])  |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (a), (take rate: 6.42))  |\n",
      "|([furniture, home furnishings and equipment shops, and manufacturers, except appliances], [a], [take rate: 6.42])  |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))  |\n",
      "|[(furniture, home furnishings and equipment shops, and manufacturers, except appliances), (b), (take rate: 3.29)]  |\n",
      "|[(furniture, home furnishings and equipment shops, and manufacturers, except appliances), (c), (take rate: 2.17)]  |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (a), (take rate: 6.94))  |\n",
      "|([furniture, home furnishings and equipment shops, and manufacturers, except appliances], [b], [take rate: 4.43])  |\n",
      "|((furniture, home furnisHings and equipment shops, and manufacturers, except appliances), (b), (take rate: 4.31))  |\n",
      "|[[furniture, home fuRnishings and equipment shops, and manufacturers, except appliances], [b], [take rate: 4.58]]  |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (b), (take rate: 4.83))  |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (b), (take rate: 4.99))  |\n",
      "|((furniture, home furnishings aNd equipment shops, and manufacturers, except appliances), (c), (take rate: 2.73))  |\n",
      "|[[furniture, home furnishings And equipment shops, and manufacturers, except appliances], [a], [take rate: 5.92]]  |\n",
      "|([furniture, home furnishings and eqUipment shops, and manufacturers, except appLiances], [b], [take rate: 4.16])  |\n",
      "|([furniture, home furnishings and equipment shops, and manufacturers, except appliances], [a], [take rate: 5.73])  |\n",
      "|[(furniture, home furnishings and equipment shops, and manufacturers, except appliAnces), (b), (take rate: 4.62)]  |\n",
      "|[(furniture, home furnishings and Equipment shops, and manufacturers, except appliances), (a), (take rate: 6.34)]  |\n",
      "|[(furniture, home furnishings and equipment shops, and manufacturers, except appliances), (b), (take rate: 4.87)]  |\n",
      "|[(furniture, home furnishings and equipment shopS, and manufacturers, except appliances), (a), (take rate: 6.61)]  |\n",
      "|[[furniture, home furnishings and equipment shops, and manufacturers, excepT appliances], [a], [take rate: 6.55]]  |\n",
      "|[(furniture, home furnishings and equipment shops, and manufacturers, except appliances), (b), (take rate: 3.78)]  |\n",
      "|[[furniture, home furnishings and equipment shOps, and manufacturers, except appliances], [d], [take rate: 0.57]]  |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (c), (take rate: 2.13))  |\n",
      "|([furniture, home furnishings and equipment shops, and manufActurers, except appliances], [c], [take rate: 2.68])  |\n",
      "|((furniture, home furnishings and equipmeNt shops, and manufacturers, except appliances), (a), (take rate: 5.52))  |\n",
      "|([furniture, home furnishings and equipmeNt shops, and manufacturers, except appliances], [b], [take rate: 4.80])  |\n",
      "|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (b), (take rate: 3.26))  |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 50 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_abn</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10023283211</td>\n",
       "      <td>((furniture, home furnishings and equipment sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10142254217</td>\n",
       "      <td>([cable, satellite, and otHer pay television a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10165489824</td>\n",
       "      <td>([jewelry, watch, clock, and silverware shops]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10187291046</td>\n",
       "      <td>([wAtch, clock, and jewelry repair shops], [b]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10192359162</td>\n",
       "      <td>([music shops - musical instruments, pianos, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   merchant_abn                                               tags\n",
       "0   10023283211  ((furniture, home furnishings and equipment sh...\n",
       "1   10142254217  ([cable, satellite, and otHer pay television a...\n",
       "2   10165489824  ([jewelry, watch, clock, and silverware shops]...\n",
       "3   10187291046  ([wAtch, clock, and jewelry repair shops], [b]...\n",
       "4   10192359162  ([music shops - musical instruments, pianos, a..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check a few of the longest tags to confirm their composition\n",
    "(\n",
    "  tbl_merchants\n",
    "  .select(\"name\",\"merchant_abn\",\"tags\", F.length(\"tags\").alias(\"len\"))\n",
    "  .orderBy(F.desc(\"len\"))\n",
    "  .show(10, truncate=False)\n",
    ")\n",
    "\n",
    "# Look at the representatives of different tag styles\n",
    "tbl_merchants.select(\"tags\").distinct().orderBy(F.length(\"tags\").desc()).show(50, truncate=False)\n",
    "\n",
    "tbl_merchants.select(\"merchant_abn\",\"tags\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c60725ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4026"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_merchants.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67ad5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicate (user_id, order_date): 99\n",
      "duplicate (merchant_abn, order_date): 0\n"
     ]
    }
   ],
   "source": [
    "### Check Abnormal Value \n",
    "dup_cf = (consumer_fraud\n",
    "          .groupBy(\"user_id\",\"order_date\")\n",
    "          .agg(Fcount(\"*\").alias(\"n\"))\n",
    "          .where(\"n>1\")\n",
    "          .count())\n",
    "dup_mf = (merchant_fraud\n",
    "          .groupBy(\"merchant_abn\",\"order_date\")\n",
    "          .agg(Fcount(\"*\").alias(\"n\"))\n",
    "          .where(\"n>1\")\n",
    "          .count())\n",
    "print(\"duplicate (user_id, order_date):\", dup_cf)\n",
    "print(\"duplicate (merchant_abn, order_date):\", dup_mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d13ac",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8eb77b",
   "metadata": {},
   "source": [
    "### tbl_consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4dd77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### tbl_consumer的gender: Undisclosed (weight = 10%)\n",
    "new_tbl_consumer = (\n",
    "    new_tbl_consumer\n",
    "    .withColumn(\n",
    "        \"gender\",\n",
    "        F.when(F.lower(F.col(\"gender\")).isin(\"male\", \"m\"), \"Male\")\n",
    "         .when(F.lower(F.col(\"gender\")).isin(\"female\", \"f\"), \"Female\")\n",
    "         .otherwise(\"Other\")   \n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ac83ed",
   "metadata": {},
   "source": [
    "### tbl_merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "791a7b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------------------------------------------------------------------------------------------------------------+------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "|name                                |tags                                                                                                             |merchant_abn|tags_lc                                                                                                          |tags_no_tr                                                                                      |tags_no_br                                                                                      |tags_sp                                                                                      |tags_norm                                                                            |\n",
      "+------------------------------------+-----------------------------------------------------------------------------------------------------------------+------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "|Felis Limited                       |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|10023283211 |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), )|  furniture, home furnishings and equipment shops, and manufacturers, except appliances ,  e ,  | furniture, home furnishings and equipment shops, and manufacturers, except appliances , e , |furniture,home furnishings and equipment shops,and manufacturers,except appliances,e,|\n",
      "|Arcu Ac Orci Corporation            |([cable, satellite, and otHer pay television and radio services], [b], [take rate: 4.22])                        |10142254217 |([cable, satellite, and other pay television and radio services], [b], [take rate: 4.22])                        |([cable, satellite, and other pay television and radio services], [b], [])                      |  cable, satellite, and other pay television and radio services ,  b ,                          | cable, satellite, and other pay television and radio services , b ,                         |cable,satellite,and other pay television and radio services,b,                       |\n",
      "|Nunc Sed Company                    |([jewelry, watch, clock, and silverware shops], [b], [take rate: 4.40])                                          |10165489824 |([jewelry, watch, clock, and silverware shops], [b], [take rate: 4.40])                                          |([jewelry, watch, clock, and silverware shops], [b], [])                                        |  jewelry, watch, clock, and silverware shops ,  b ,                                            | jewelry, watch, clock, and silverware shops , b ,                                           |jewelry,watch,clock,and silverware shops,b,                                          |\n",
      "|Ultricies Dignissim Lacus Foundation|([wAtch, clock, and jewelry repair shops], [b], [take rate: 3.29])                                               |10187291046 |([watch, clock, and jewelry repair shops], [b], [take rate: 3.29])                                               |([watch, clock, and jewelry repair shops], [b], [])                                             |  watch, clock, and jewelry repair shops ,  b ,                                                 | watch, clock, and jewelry repair shops , b ,                                                |watch,clock,and jewelry repair shops,b,                                              |\n",
      "|Enim Condimentum PC                 |([music shops - musical instruments, pianos, and sheet music], [a], [take rate: 6.33])                           |10192359162 |([music shops - musical instruments, pianos, and sheet music], [a], [take rate: 6.33])                           |([music shops - musical instruments, pianos, and sheet music], [a], [])                         |  music shops - musical instruments, pianos, and sheet music ,  a ,                             | music shops - musical instruments, pianos, and sheet music , a ,                            |music shops - musical instruments,pianos,and sheet music,a,                          |\n",
      "|Fusce Company                       |[(gift, card, novelty, and souvenir shops), (a), (take rate: 6.34)]                                              |10206519221 |[(gift, card, novelty, and souvenir shops), (a), (take rate: 6.34)]                                              |[(gift, card, novelty, and souvenir shops), (a), ]                                              |  gift, card, novelty, and souvenir shops ,  a ,                                                | gift, card, novelty, and souvenir shops , a ,                                               |gift,card,novelty,and souvenir shops,a,                                              |\n",
      "|Aliquam Enim Incorporated           |[(computers, comPUter peripheral equipment, and softwAre), (b), (take rate: 4.32)]                               |10255988167 |[(computers, computer peripheral equipment, and software), (b), (take rate: 4.32)]                               |[(computers, computer peripheral equipment, and software), (b), ]                               |  computers, computer peripheral equipment, and software ,  b ,                                 | computers, computer peripheral equipment, and software , b ,                                |computers,computer peripheral equipment,and software,b,                              |\n",
      "|Ipsum Primis Ltd                    |[[watch, clock, and jewelry repair shops], [c], [take rate: 2.39]]                                               |10264435225 |[[watch, clock, and jewelry repair shops], [c], [take rate: 2.39]]                                               |[[watch, clock, and jewelry repair shops], [c], []]                                             |  watch, clock, and jewelry repair shops ,  c ,                                                 | watch, clock, and jewelry repair shops , c ,                                                |watch,clock,and jewelry repair shops,c,                                              |\n",
      "|Pede Ultrices Industries            |([computer programming , data processing, and integrated systems design services], [a], [take rate: 5.71])       |10279061213 |([computer programming , data processing, and integrated systems design services], [a], [take rate: 5.71])       |([computer programming , data processing, and integrated systems design services], [a], [])     |  computer programming , data processing, and integrated systems design services ,  a ,         | computer programming , data processing, and integrated systems design services , a ,        |computer programming,data processing,and integrated systems design services,a,       |\n",
      "|Nunc Inc.                           |[(furniture, home furnishings and equipment shopS, and manufacturers, except appliances), (a), (take rate: 6.61)]|10323485998 |[(furniture, home furnishings and equipment shops, and manufacturers, except appliances), (a), (take rate: 6.61)]|[(furniture, home furnishings and equipment shops, and manufacturers, except appliances), (a), ]|  furniture, home furnishings and equipment shops, and manufacturers, except appliances ,  a ,  | furniture, home furnishings and equipment shops, and manufacturers, except appliances , a , |furniture,home furnishings and equipment shops,and manufacturers,except appliances,a,|\n",
      "+------------------------------------+-----------------------------------------------------------------------------------------------------------------+------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Uniform lowercase\n",
    "m1 = tbl_merchants.withColumn(\"tags_lc\", F.lower(F.col(\"tags\")))\n",
    "\n",
    "# Remove 'take rate: x.xx' and possible outer parentheses, commas before and after, and Spaces\n",
    "m2 = m1.withColumn(\n",
    "    \"tags_no_tr\",\n",
    "    F.regexp_replace(\n",
    "        F.col(\"tags_lc\"),\n",
    "        r\"\\(?\\s*take\\s*rate\\s*:\\s*\\d+(\\.\\d+)?\\s*\\)?\\s*,?\",  \n",
    "        \"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Remove all parentheses, square brackets and double brackets, and press all kinds of blanks into single Spaces\n",
    "m3 = (m2\n",
    "    .withColumn(\"tags_no_br\", F.regexp_replace(\"tags_no_tr\", r\"[\\[\\]\\(\\)]\", \" \"))\n",
    "    .withColumn(\"tags_sp\",   F.regexp_replace(\"tags_no_br\",  r\"\\s+\", \" \"))     \n",
    "    .withColumn(\"tags_norm\", F.trim(F.regexp_replace(\"tags_sp\", r\"\\s*,\\s*\", \",\")))\n",
    ")\n",
    "\n",
    "m3.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a11e1e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------------------------------------------------------------------------------------------------+------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+-------------------------------------------+\n",
      "|name                    |tags                                                                                                             |merchant_abn|tags_lc                                                                                                          |tags_no_tr                                                                                      |tags_no_br                                                                                      |tags_sp                                                                                      |tags_norm                                                                            |tag_arr                                                                                     |tag_token                                  |\n",
      "+------------------------+-----------------------------------------------------------------------------------------------------------------+------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+-------------------------------------------+\n",
      "|Felis Limited           |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|10023283211 |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), )|  furniture, home furnishings and equipment shops, and manufacturers, except appliances ,  e ,  | furniture, home furnishings and equipment shops, and manufacturers, except appliances , e , |furniture,home furnishings and equipment shops,and manufacturers,except appliances,e,|[furniture, home furnishings and equipment shops, and manufacturers, except appliances, e, ]|furniture                                  |\n",
      "|Felis Limited           |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|10023283211 |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), )|  furniture, home furnishings and equipment shops, and manufacturers, except appliances ,  e ,  | furniture, home furnishings and equipment shops, and manufacturers, except appliances , e , |furniture,home furnishings and equipment shops,and manufacturers,except appliances,e,|[furniture, home furnishings and equipment shops, and manufacturers, except appliances, e, ]|home furnishings and equipment shops       |\n",
      "|Felis Limited           |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|10023283211 |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), )|  furniture, home furnishings and equipment shops, and manufacturers, except appliances ,  e ,  | furniture, home furnishings and equipment shops, and manufacturers, except appliances , e , |furniture,home furnishings and equipment shops,and manufacturers,except appliances,e,|[furniture, home furnishings and equipment shops, and manufacturers, except appliances, e, ]|and manufacturers                          |\n",
      "|Felis Limited           |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|10023283211 |((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), (take rate: 0.18))|((furniture, home furnishings and equipment shops, and manufacturers, except appliances), (e), )|  furniture, home furnishings and equipment shops, and manufacturers, except appliances ,  e ,  | furniture, home furnishings and equipment shops, and manufacturers, except appliances , e , |furniture,home furnishings and equipment shops,and manufacturers,except appliances,e,|[furniture, home furnishings and equipment shops, and manufacturers, except appliances, e, ]|except appliances                          |\n",
      "|Arcu Ac Orci Corporation|([cable, satellite, and otHer pay television and radio services], [b], [take rate: 4.22])                        |10142254217 |([cable, satellite, and other pay television and radio services], [b], [take rate: 4.22])                        |([cable, satellite, and other pay television and radio services], [b], [])                      |  cable, satellite, and other pay television and radio services ,  b ,                          | cable, satellite, and other pay television and radio services , b ,                         |cable,satellite,and other pay television and radio services,b,                       |[cable, satellite, and other pay television and radio services, b, ]                        |cable                                      |\n",
      "|Arcu Ac Orci Corporation|([cable, satellite, and otHer pay television and radio services], [b], [take rate: 4.22])                        |10142254217 |([cable, satellite, and other pay television and radio services], [b], [take rate: 4.22])                        |([cable, satellite, and other pay television and radio services], [b], [])                      |  cable, satellite, and other pay television and radio services ,  b ,                          | cable, satellite, and other pay television and radio services , b ,                         |cable,satellite,and other pay television and radio services,b,                       |[cable, satellite, and other pay television and radio services, b, ]                        |satellite                                  |\n",
      "|Arcu Ac Orci Corporation|([cable, satellite, and otHer pay television and radio services], [b], [take rate: 4.22])                        |10142254217 |([cable, satellite, and other pay television and radio services], [b], [take rate: 4.22])                        |([cable, satellite, and other pay television and radio services], [b], [])                      |  cable, satellite, and other pay television and radio services ,  b ,                          | cable, satellite, and other pay television and radio services , b ,                         |cable,satellite,and other pay television and radio services,b,                       |[cable, satellite, and other pay television and radio services, b, ]                        |and other pay television and radio services|\n",
      "|Nunc Sed Company        |([jewelry, watch, clock, and silverware shops], [b], [take rate: 4.40])                                          |10165489824 |([jewelry, watch, clock, and silverware shops], [b], [take rate: 4.40])                                          |([jewelry, watch, clock, and silverware shops], [b], [])                                        |  jewelry, watch, clock, and silverware shops ,  b ,                                            | jewelry, watch, clock, and silverware shops , b ,                                           |jewelry,watch,clock,and silverware shops,b,                                          |[jewelry, watch, clock, and silverware shops, b, ]                                          |jewelry                                    |\n",
      "|Nunc Sed Company        |([jewelry, watch, clock, and silverware shops], [b], [take rate: 4.40])                                          |10165489824 |([jewelry, watch, clock, and silverware shops], [b], [take rate: 4.40])                                          |([jewelry, watch, clock, and silverware shops], [b], [])                                        |  jewelry, watch, clock, and silverware shops ,  b ,                                            | jewelry, watch, clock, and silverware shops , b ,                                           |jewelry,watch,clock,and silverware shops,b,                                          |[jewelry, watch, clock, and silverware shops, b, ]                                          |watch                                      |\n",
      "|Nunc Sed Company        |([jewelry, watch, clock, and silverware shops], [b], [take rate: 4.40])                                          |10165489824 |([jewelry, watch, clock, and silverware shops], [b], [take rate: 4.40])                                          |([jewelry, watch, clock, and silverware shops], [b], [])                                        |  jewelry, watch, clock, and silverware shops ,  b ,                                            | jewelry, watch, clock, and silverware shops , b ,                                           |jewelry,watch,clock,and silverware shops,b,                                          |[jewelry, watch, clock, and silverware shops, b, ]                                          |clock                                      |\n",
      "+------------------------+-----------------------------------------------------------------------------------------------------------------+------------+-----------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+-------------------------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Split by comma, explode to rows\n",
    "m_tokens = (m3\n",
    "    .withColumn(\"tag_arr\",  F.split(F.col(\"tags_norm\"), r\",\"))\n",
    "    .withColumn(\"tag_token\", F.explode(F.col(\"tag_arr\")))\n",
    "    .withColumn(\"tag_token\", F.trim(F.col(\"tag_token\")))\n",
    ")\n",
    "\n",
    "# Filter out obvious noise: empty strings, single letters, pure numbers/pure decimals\n",
    "m_tokens_clean = (m_tokens\n",
    "    .filter(F.col(\"tag_token\") != \"\")\n",
    "    .filter(~F.col(\"tag_token\").rlike(r\"^[a-z]$\"))             \n",
    "    .filter(~F.col(\"tag_token\").rlike(r\"^\\d+(\\.\\d+)?$\"))      \n",
    ")\n",
    "\n",
    "m_tokens_clean = (m_tokens_clean\n",
    "    .filter(F.col(\"name\").isNotNull())\n",
    ")\n",
    "\n",
    "m_tokens_clean.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b75b3198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "|name                                |merchant_abn|tag_tokens                                                                             |tags_clean                                                                           |\n",
      "+------------------------------------+------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "|Felis Limited                       |10023283211 |[and manufacturers, except appliances, furniture, home furnishings and equipment shops]|and manufacturers, except appliances, furniture, home furnishings and equipment shops|\n",
      "|Arcu Ac Orci Corporation            |10142254217 |[and other pay television and radio services, cable, satellite]                        |and other pay television and radio services, cable, satellite                        |\n",
      "|Nunc Sed Company                    |10165489824 |[and silverware shops, clock, jewelry, watch]                                          |and silverware shops, clock, jewelry, watch                                          |\n",
      "|Ultricies Dignissim Lacus Foundation|10187291046 |[and jewelry repair shops, clock, watch]                                               |and jewelry repair shops, clock, watch                                               |\n",
      "|Enim Condimentum PC                 |10192359162 |[and sheet music, music shops - musical instruments, pianos]                           |and sheet music, music shops - musical instruments, pianos                           |\n",
      "|Fusce Company                       |10206519221 |[and souvenir shops, card, gift, novelty]                                              |and souvenir shops, card, gift, novelty                                              |\n",
      "|Aliquam Enim Incorporated           |10255988167 |[and software, computer peripheral equipment, computers]                               |and software, computer peripheral equipment, computers                               |\n",
      "|Ipsum Primis Ltd                    |10264435225 |[and jewelry repair shops, clock, watch]                                               |and jewelry repair shops, clock, watch                                               |\n",
      "|Pede Ultrices Industries            |10279061213 |[and integrated systems design services, computer programming, data processing]        |and integrated systems design services, computer programming, data processing        |\n",
      "|Nunc Inc.                           |10323485998 |[and manufacturers, except appliances, furniture, home furnishings and equipment shops]|and manufacturers, except appliances, furniture, home furnishings and equipment shops|\n",
      "|Facilisis Facilisis Corp.           |10342410215 |[and software, computer peripheral equipment, computers]                               |and software, computer peripheral equipment, computers                               |\n",
      "|Odio Institute                      |10346855916 |[and appliance rent al and leasing, equipment, furniture, tool]                        |and appliance rent al and leasing, equipment, furniture, tool                        |\n",
      "|Rutrum Justo Ltd                    |10364012396 |[and sheet music, music shops - musical instruments, pianos]                           |and sheet music, music shops - musical instruments, pianos                           |\n",
      "|Tellus Foundation                   |10385011947 |[artist supply and craft shops]                                                        |artist supply and craft shops                                                        |\n",
      "|Sed Et Company                      |10385163239 |[and flowers, florists supplies, nursery stock]                                        |and flowers, florists supplies, nursery stock                                        |\n",
      "|Id Ltd                              |10385250025 |[and software, computer peripheral equipment, computers]                               |and software, computer peripheral equipment, computers                               |\n",
      "|Consequat Foundation                |10404542215 |[and restoration services, antique shops - sales, repairs]                             |and restoration services, antique shops - sales, repairs                             |\n",
      "|Sit Amet Nulla Corp.                |10430380319 |[motor vehicle supplies and new parts]                                                 |motor vehicle supplies and new parts                                                 |\n",
      "|Massa Vestibulum Foundation         |10441711491 |[motor vehicle supplies and new parts]                                                 |motor vehicle supplies and new parts                                                 |\n",
      "|Ut Consulting                       |10462560289 |[and souvenir shops, card, gift, novelty]                                              |and souvenir shops, card, gift, novelty                                              |\n",
      "+------------------------------------+------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Each merchant aggregates the de-duplicated tokens and reassembles them into a standardized string\n",
    "merchant_base = tbl_merchants.select(\"merchant_abn\", \"name\")\n",
    "\n",
    "tbl_merchants_clean = (\n",
    "    m_tokens_clean\n",
    "      .groupBy(\"merchant_abn\")\n",
    "      .agg(\n",
    "          F.first(\"name\").alias(\"name\"),\n",
    "          F.array_sort(F.array_distinct(F.collect_list(\"tag_token\"))).alias(\"tag_tokens\")\n",
    "      )\n",
    "      .withColumn(\"tags_clean\", F.array_join(F.col(\"tag_tokens\"), \", \"))\n",
    "      .select(\"name\", \"merchant_abn\", \"tag_tokens\", \"tags_clean\")\n",
    ")\n",
    "\n",
    "# Show the cleaned result\n",
    "tbl_merchants_clean.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e0583e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4026"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_merchants_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+-----+\n",
      "|tag_token                                     |count|\n",
      "+----------------------------------------------+-----+\n",
      "|furniture                                     |316  |\n",
      "|watch                                         |261  |\n",
      "|clock                                         |261  |\n",
      "|digital goods: books                          |195  |\n",
      "|music                                         |195  |\n",
      "|movies                                        |195  |\n",
      "|artist supply and craft shops                 |193  |\n",
      "|and integrated systems design services        |191  |\n",
      "|data processing                               |191  |\n",
      "|computer programming                          |191  |\n",
      "|shoe shops                                    |185  |\n",
      "|and souvenir shops                            |182  |\n",
      "|novelty                                       |182  |\n",
      "|home furnishings and equipment shops          |182  |\n",
      "|card                                          |182  |\n",
      "|except appliances                             |182  |\n",
      "|gift                                          |182  |\n",
      "|and manufacturers                             |182  |\n",
      "|computers                                     |181  |\n",
      "|computer peripheral equipment                 |181  |\n",
      "|and software                                  |181  |\n",
      "|florists supplies                             |180  |\n",
      "|and flowers                                   |180  |\n",
      "|nursery stock                                 |180  |\n",
      "|tent and awning shops                         |178  |\n",
      "|cable                                         |175  |\n",
      "|and other pay television and radio services   |175  |\n",
      "|satellite                                     |175  |\n",
      "|and jewelry repair shops                      |170  |\n",
      "|bicycle shops - sales and service             |170  |\n",
      "|music shops - musical instruments             |167  |\n",
      "|and sheet music                               |167  |\n",
      "|pianos                                        |167  |\n",
      "|periodicals                                   |164  |\n",
      "|books                                         |164  |\n",
      "|and newspapers                                |164  |\n",
      "|health and beauty spas                        |164  |\n",
      "|office supplies and printing and writing paper|161  |\n",
      "|stationery                                    |161  |\n",
      "|including nurseries                           |153  |\n",
      "|lawn and garden supply outlets                |153  |\n",
      "|and eyeglasses                                |151  |\n",
      "|opticians                                     |151  |\n",
      "|optical goods                                 |151  |\n",
      "|motor vehicle supplies and new parts          |151  |\n",
      "|toy and game shops                            |142  |\n",
      "|hobby                                         |142  |\n",
      "|tool                                          |134  |\n",
      "|and appliance rent al and leasing             |134  |\n",
      "|equipment                                     |134  |\n",
      "|antique shops - sales                         |129  |\n",
      "|and restoration services                      |129  |\n",
      "|repairs                                       |129  |\n",
      "|telecom                                       |125  |\n",
      "|art dealers and galleries                     |112  |\n",
      "|jewelry                                       |91   |\n",
      "|and silverware shops                          |91   |\n",
      "+----------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the top 50 token frequencies to help you determine the Layer 2 mapping vocabulary\n",
    "token_freq = (m_tokens_clean\n",
    "    .groupBy(\"tag_token\").count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "token_freq.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cde9a62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "|name                                |merchant_abn|tag_tokens                                                                             |tags_clean                                                                           |\n",
      "+------------------------------------+------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "|Felis Limited                       |10023283211 |[and manufacturers, except appliances, furniture, home furnishings and equipment shops]|and manufacturers, except appliances, furniture, home furnishings and equipment shops|\n",
      "|Arcu Ac Orci Corporation            |10142254217 |[and other pay television and radio services, cable, satellite]                        |and other pay television and radio services, cable, satellite                        |\n",
      "|Nunc Sed Company                    |10165489824 |[and silverware shops, clock, jewelry, watch]                                          |and silverware shops, clock, jewelry, watch                                          |\n",
      "|Ultricies Dignissim Lacus Foundation|10187291046 |[and jewelry repair shops, clock, watch]                                               |and jewelry repair shops, clock, watch                                               |\n",
      "|Enim Condimentum PC                 |10192359162 |[and sheet music, music shops - musical instruments, pianos]                           |and sheet music, music shops - musical instruments, pianos                           |\n",
      "|Fusce Company                       |10206519221 |[and souvenir shops, card, gift, novelty]                                              |and souvenir shops, card, gift, novelty                                              |\n",
      "|Aliquam Enim Incorporated           |10255988167 |[and software, computer peripheral equipment, computers]                               |and software, computer peripheral equipment, computers                               |\n",
      "|Ipsum Primis Ltd                    |10264435225 |[and jewelry repair shops, clock, watch]                                               |and jewelry repair shops, clock, watch                                               |\n",
      "|Pede Ultrices Industries            |10279061213 |[and integrated systems design services, computer programming, data processing]        |and integrated systems design services, computer programming, data processing        |\n",
      "|Nunc Inc.                           |10323485998 |[and manufacturers, except appliances, furniture, home furnishings and equipment shops]|and manufacturers, except appliances, furniture, home furnishings and equipment shops|\n",
      "|Facilisis Facilisis Corp.           |10342410215 |[and software, computer peripheral equipment, computers]                               |and software, computer peripheral equipment, computers                               |\n",
      "|Odio Institute                      |10346855916 |[and appliance rent al and leasing, equipment, furniture, tool]                        |and appliance rent al and leasing, equipment, furniture, tool                        |\n",
      "|Rutrum Justo Ltd                    |10364012396 |[and sheet music, music shops - musical instruments, pianos]                           |and sheet music, music shops - musical instruments, pianos                           |\n",
      "|Tellus Foundation                   |10385011947 |[artist supply and craft shops]                                                        |artist supply and craft shops                                                        |\n",
      "|Sed Et Company                      |10385163239 |[and flowers, florists supplies, nursery stock]                                        |and flowers, florists supplies, nursery stock                                        |\n",
      "|Id Ltd                              |10385250025 |[and software, computer peripheral equipment, computers]                               |and software, computer peripheral equipment, computers                               |\n",
      "|Consequat Foundation                |10404542215 |[and restoration services, antique shops - sales, repairs]                             |and restoration services, antique shops - sales, repairs                             |\n",
      "|Sit Amet Nulla Corp.                |10430380319 |[motor vehicle supplies and new parts]                                                 |motor vehicle supplies and new parts                                                 |\n",
      "|Massa Vestibulum Foundation         |10441711491 |[motor vehicle supplies and new parts]                                                 |motor vehicle supplies and new parts                                                 |\n",
      "|Ut Consulting                       |10462560289 |[and souvenir shops, card, gift, novelty]                                              |and souvenir shops, card, gift, novelty                                              |\n",
      "|Id Mollis Corporation               |10463252268 |[artist supply and craft shops]                                                        |artist supply and craft shops                                                        |\n",
      "|Laoreet Lectus Associates           |10487253336 |[artist supply and craft shops]                                                        |artist supply and craft shops                                                        |\n",
      "|Ut Semper Industries                |10530696903 |[and newspapers, books, periodicals]                                                   |and newspapers, books, periodicals                                                   |\n",
      "|Lacus Vestibulum Inc.               |10545955006 |[and flowers, florists supplies, nursery stock]                                        |and flowers, florists supplies, nursery stock                                        |\n",
      "|Commodo Hendrerit Donec Corporation |10553813474 |[artist supply and craft shops]                                                        |artist supply and craft shops                                                        |\n",
      "|Egestas A Associates                |10596295795 |[and silverware shops, clock, jewelry, watch]                                          |and silverware shops, clock, jewelry, watch                                          |\n",
      "|Enim Consequat Institute            |10618089367 |[office supplies and printing and writing paper, stationery]                           |office supplies and printing and writing paper, stationery                           |\n",
      "|Proin Nisl Institute                |10648956813 |[and software, computer peripheral equipment, computers]                               |and software, computer peripheral equipment, computers                               |\n",
      "|Sed Et Libero PC                    |10651113986 |[tent and awning shops]                                                                |tent and awning shops                                                                |\n",
      "|Fermentum Risus Foundation          |10702078694 |[and souvenir shops, card, gift, novelty]                                              |and souvenir shops, card, gift, novelty                                              |\n",
      "+------------------------------------+------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "only showing top 30 rows\n"
     ]
    }
   ],
   "source": [
    "tbl_merchants_clean = tbl_merchants_clean.drop(\"tags_tokens\")\n",
    "tbl_merchants_clean.show(30, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "688b562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tag dictionary\n",
    "tag_category_map = {\n",
    "    # Home furnishings/furniture category\n",
    "    \"furniture\": \"Home & Furniture\",\n",
    "    \"home furnishings\": \"Home & Furniture\",\n",
    "    \"equipment shops\": \"Home & Furniture\",\n",
    "    \"manufacturers\": \"Home & Furniture\",\n",
    "    \"appliances\": \"Home & Furniture\",   \n",
    "\n",
    "    # Electrical appliances/small household appliances category\n",
    "    \"except appliances\": \"Small Appliances\", \n",
    "\n",
    "    # Musical Instruments/cultural and entertainment category\n",
    "    \"music shops\": \"Culture & Entertainment\",\n",
    "    \"sheet music\": \"Culture & Entertainment\",\n",
    "    \"pianos\": \"Culture & Entertainment\",\n",
    "    \"musical instruments\": \"Culture & Entertainment\",\n",
    "\n",
    "    # Jewelry/luxury goods category\n",
    "    \"jewelry\": \"Jewellery & Watches\",\n",
    "    \"jewellery\": \"Jewellery & Watches\",\n",
    "    \"watch\": \"Jewellery & Watches\",\n",
    "    \"clock\": \"Jewellery & Watches\",\n",
    "    \"silverware\": \"Jewellery & Watches\",\n",
    "\n",
    "    # Gifts/commemorative items\n",
    "    \"souvenir\": \"Gifts & Novelties\",\n",
    "    \"gift\": \"Gifts & Novelties\",\n",
    "    \"card\": \"Gifts & Novelties\",\n",
    "    \"novelty\": \"Gifts & Novelties\",\n",
    "\n",
    "    # Software/IT/telecommunications category\n",
    "    \"software\": \"Software & IT\",\n",
    "    \"data processing\": \"Software & IT\",\n",
    "    \"computer\": \"Software & IT\",\n",
    "    \"telecom\": \"Telecom\",\n",
    "    \"telecommunications\": \"Telecom\",\n",
    "    \"mobile\": \"Telecom\",\n",
    "\n",
    "    # Handicrafts/flowers \n",
    "    \"artist supply\": \"Crafts & Flowers\",\n",
    "    \"craft shops\": \"Crafts & Flowers\",\n",
    "    \"florists\": \"Crafts & Flowers\",\n",
    "    \"nursery stock\": \"Crafts & Flowers\",\n",
    "\n",
    "    # Maintenance/leasing category\n",
    "    \"repair\": \"Repair & Leasing\",\n",
    "    \"appliance rent\": \"Repair & Leasing\",\n",
    "    \"leasing\": \"Repair & Leasing\",\n",
    "    \"tool\": \"Repair & Leasing\",\n",
    "\n",
    "    # Automotive\n",
    "    \"motor vehicle\": \"Automotive\",\n",
    "    \"new parts\": \"Automotive\"\n",
    "}\n",
    "\n",
    "# Map the token to the major category\n",
    "def _map_category(token: str):\n",
    "    if token is None:\n",
    "        return None\n",
    "    s = token.lower()\n",
    "    for k, v in tag_category_map.items():\n",
    "        if k in s:    \n",
    "            return v\n",
    "    return None          # if cannot be map, use Other\n",
    "\n",
    "map_category_udf = F.udf(_map_category, \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aad0b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the tags_clean word, standardize it and map it to the major category\n",
    "tokens_df = (\n",
    "    tbl_merchants_clean\n",
    "      .select(\"merchant_abn\", \"tags_clean\")\n",
    "      .withColumn(\"tok_arr\", F.split(F.col(\"tags_clean\"), \",\"))          # Comma separated\n",
    "      .withColumn(\"tok\", F.explode(\"tok_arr\"))                           # Split\n",
    "      .withColumn(\"tok\", F.trim(F.lower(F.col(\"tok\"))))                  # Unify lowercase letters without Spaces\n",
    "      .filter(F.col(\"tok\") != \"\")                                        # Delete the empty\n",
    "      .withColumn(\"tag_category\", map_category_udf(F.col(\"tok\")))        # Mapping major classes\n",
    "      .withColumn(\"tag_category\", F.when(F.col(\"tag_category\").isNull(), F.lit(\"Other\"))\n",
    "                                   .otherwise(F.col(\"tag_category\")))\n",
    "      .select(\"merchant_abn\", \"tag_category\")\n",
    "      .dropDuplicates()                                                  # Remove duplicates from each merchant and each category\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14a89567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrence frequency of each merchant in each major category and sort/rank them\n",
    "cat_counts = (\n",
    "    tokens_df\n",
    "      .groupBy(\"merchant_abn\", \"tag_category\")\n",
    "      .agg(F.count(F.lit(1)).alias(\"n\"))\n",
    "      .withColumn(\"is_other\", F.when(F.col(\"tag_category\") == \"Other\", F.lit(1)).otherwise(F.lit(0)))\n",
    ")\n",
    "\n",
    "# Sorting strategy: In descending order of occurrence frequency; In the same number of times, non-other takes priority. Then, in ascending order by category name\n",
    "w = Window.partitionBy(\"merchant_abn\").orderBy(\n",
    "        F.desc(\"n\"),\n",
    "        F.asc(\"is_other\"),\n",
    "        F.asc(\"tag_category\")\n",
    "    )\n",
    "\n",
    "ranked = cat_counts.withColumn(\"rk\", F.row_number().over(w))\n",
    "\n",
    "top1 = (\n",
    "    ranked\n",
    "      .filter(F.col(\"rk\") == 1)\n",
    "      .select(\"merchant_abn\", F.col(\"tag_category\").alias(\"tag_category_top\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72c07b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------+-----------------------+\n",
      "|name                                |merchant_abn|tag_category_top       |\n",
      "+------------------------------------+------------+-----------------------+\n",
      "|Felis Limited                       |10023283211 |Home & Furniture       |\n",
      "|Arcu Ac Orci Corporation            |10142254217 |Other                  |\n",
      "|Nunc Sed Company                    |10165489824 |Jewellery & Watches    |\n",
      "|Ultricies Dignissim Lacus Foundation|10187291046 |Jewellery & Watches    |\n",
      "|Enim Condimentum PC                 |10192359162 |Culture & Entertainment|\n",
      "|Fusce Company                       |10206519221 |Gifts & Novelties      |\n",
      "|Aliquam Enim Incorporated           |10255988167 |Software & IT          |\n",
      "|Ipsum Primis Ltd                    |10264435225 |Jewellery & Watches    |\n",
      "|Pede Ultrices Industries            |10279061213 |Software & IT          |\n",
      "|Nunc Inc.                           |10323485998 |Home & Furniture       |\n",
      "+------------------------------------+------------+-----------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Write back to the merchant table\n",
    "tbl_merchants_enriched = (\n",
    "    tbl_merchants_clean\n",
    "        .join(top1, \"merchant_abn\", \"left\")\n",
    ")\n",
    "\n",
    "tbl_merchants_enriched = tbl_merchants_enriched.filter(F.col(\"tag_category_top\").isNotNull())\n",
    "\n",
    "\n",
    "# Show the result\n",
    "tbl_merchants_enriched.select(\n",
    "    \"name\", \"merchant_abn\", \"tag_category_top\"\n",
    ").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fdd70696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4026"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_merchants_enriched.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3a7e820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|merchant_abn|                name|          tag_tokens|          tags_clean|    tag_category_top|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 10023283211|       Felis Limited|[and manufacturer...|and manufacturers...|    Home & Furniture|\n",
      "| 10142254217|Arcu Ac Orci Corp...|[and other pay te...|and other pay tel...|               Other|\n",
      "| 10165489824|    Nunc Sed Company|[and silverware s...|and silverware sh...| Jewellery & Watches|\n",
      "| 10187291046|Ultricies Digniss...|[and jewelry repa...|and jewelry repai...| Jewellery & Watches|\n",
      "| 10192359162| Enim Condimentum PC|[and sheet music,...|and sheet music, ...|Culture & Enterta...|\n",
      "| 10206519221|       Fusce Company|[and souvenir sho...|and souvenir shop...|   Gifts & Novelties|\n",
      "| 10255988167|Aliquam Enim Inco...|[and software, co...|and software, com...|       Software & IT|\n",
      "| 10264435225|    Ipsum Primis Ltd|[and jewelry repa...|and jewelry repai...| Jewellery & Watches|\n",
      "| 10279061213|Pede Ultrices Ind...|[and integrated s...|and integrated sy...|       Software & IT|\n",
      "| 10323485998|           Nunc Inc.|[and manufacturer...|and manufacturers...|    Home & Furniture|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "tbl_merchants_enriched.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91831f0f",
   "metadata": {},
   "source": [
    "# Join Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b670dab",
   "metadata": {},
   "source": [
    "### Consumer Level "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d78a4f",
   "metadata": {},
   "source": [
    "##### consumer_fraud + consumer_details + tbl_consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4f87311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> consumer_joined preview\n",
      "+-------+-----------+----------+-----------------+-------+----------+--------+\n",
      "|user_id|consumer_id|order_date|c_fraud_prob     |c_state|c_postcode|c_gender|\n",
      "+-------+-----------+----------+-----------------+-------+----------+--------+\n",
      "|21419  |10448      |2021-12-10|99.24738020302328|QLD    |4003      |Female  |\n",
      "|6228   |1242133    |2021-12-19|97.6298077657765 |VIC    |3186      |Male    |\n",
      "|22239  |184299     |2021-10-19|94.70342477508035|VIC    |3813      |Male    |\n",
      "|5606   |882166     |2021-10-17|84.05825045251777|QLD    |4454      |Other   |\n",
      "|16556  |1243659    |2022-02-20|89.65663294494827|QLD    |4743      |Male    |\n",
      "+-------+-----------+----------+-----------------+-------+----------+--------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- consumer_id: long (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- order_datetime: date (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- is_prob_out_of_range: integer (nullable = false)\n",
      " |-- c_fraud_prob: double (nullable = true)\n",
      " |-- c_state: string (nullable = true)\n",
      " |-- c_postcode: integer (nullable = true)\n",
      " |-- c_gender: string (nullable = true)\n",
      "\n",
      "[Check] consumer_id is NULL after join: 0\n"
     ]
    }
   ],
   "source": [
    "### Consumer level join \n",
    "\n",
    "# Type/Column processing\n",
    "cf = (\n",
    "    consumer_fraud\n",
    "    .withColumn(\"order_date\", to_date(col(\"order_datetime\")))  \n",
    "    .withColumn(\"c_fraud_prob\", col(\"fraud_probability\").cast(\"double\"))  # rename the fraud column\n",
    "    .drop(\"fraud_probability\")  # delete the old column\n",
    ")\n",
    "\n",
    "# user_id -> consumer_id (Deduplication: If user_id maps multiple Consumerids, only one is retained)\n",
    "cd = (\n",
    "    consumer_details\n",
    "    .select(\"user_id\", \"consumer_id\")\n",
    "    .dropDuplicates([\"user_id\"])  \n",
    ")\n",
    "\n",
    "# User profile (To avoid column conflicts, prefix the profile field with \"c_\")\n",
    "tc = (\n",
    "    new_tbl_consumer\n",
    "    .select(\n",
    "        col(\"consumer_id\"),\n",
    "        col(\"state\").alias(\"c_state\"),\n",
    "        col(\"postcode\").alias(\"c_postcode\"),\n",
    "        col(\"gender\").alias(\"c_gender\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# left join: Transaction mapping portrait in sequence\n",
    "consumer_joined = (\n",
    "    cf.alias(\"cf\")\n",
    "      .join(cd.alias(\"cd\"), on=\"user_id\", how=\"left\")\n",
    "      .join(tc.alias(\"tc\"), on=\"consumer_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Preview the result\n",
    "print(\">>> consumer_joined preview\")\n",
    "consumer_joined.select(\"user_id\", \"consumer_id\", \"order_date\", \"c_fraud_prob\", \"c_state\", \"c_postcode\", \"c_gender\").show(5, truncate=False)\n",
    "consumer_joined.printSchema()\n",
    "\n",
    "consumer_level = consumer_joined.filter(col(\"consumer_id\").isNull()).count()\n",
    "print(f\"[Check] consumer_id is NULL after join: {consumer_level}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bca0a3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consumer_profile saved to: part1_data/consumer_profile.parquet\n",
      "+---------+-----------+------------------+-------+\n",
      "|c_user_id|consumer_id|c_fraud_prob      |c_state|\n",
      "+---------+-----------+------------------+-------+\n",
      "|13842    |30         |16.709045232443952|VIC    |\n",
      "|15373    |105        |12.319746480481419|NSW    |\n",
      "|5162     |226        |8.91637740763569  |SA     |\n",
      "|2979     |257        |12.305320973481981|VIC    |\n",
      "|22602    |438        |12.319612170395782|NSW    |\n",
      "+---------+-----------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "rows: 20128\n"
     ]
    }
   ],
   "source": [
    "# Select the \"No Time\" field you need to keep and name it uniformly in lowercase\n",
    "consumer_profile = (\n",
    "    consumer_joined\n",
    "    .select(\n",
    "        F.col(\"user_id\").alias(\"c_user_id\"),\n",
    "        F.col(\"consumer_id\").alias(\"consumer_id\"),\n",
    "        F.col(\"c_fraud_prob\").alias(\"c_fraud_prob\"),\n",
    "        F.col(\"c_state\")\n",
    "    )\n",
    "    .dropDuplicates([\"consumer_id\"])       \n",
    ")\n",
    "\n",
    "# Save the file and output\n",
    "out_path = \"part1_data/consumer_profile.parquet\"\n",
    "consumer_profile.write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "print(f\"consumer_profile saved to: {out_path}\")\n",
    "consumer_profile.show(5, truncate=False)\n",
    "print(\"rows:\", consumer_profile.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95597f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer profile saved to part1_data/consumer_profile.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save consumer_joined\n",
    "consumer_path = \"part1_data/consumer_profile.parquet\"\n",
    "consumer_profile.write.mode(\"overwrite\").parquet(consumer_path)\n",
    "print(f\"Consumer profile saved to {consumer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790ec93",
   "metadata": {},
   "source": [
    "### Merchant Level "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26dafb",
   "metadata": {},
   "source": [
    "##### merchant_fraud_probability + tbl_merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3fc258ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------------+-----------------------+----------+------------+\n",
      "|merchant_abn|m_name                              |m_category             |order_date|m_fraud_prob|\n",
      "+------------+------------------------------------+-----------------------+----------+------------+\n",
      "|10023283211 |Felis Limited                       |Home & Furniture       |NULL      |NULL        |\n",
      "|10142254217 |Arcu Ac Orci Corporation            |Other                  |NULL      |NULL        |\n",
      "|10165489824 |Nunc Sed Company                    |Jewellery & Watches    |NULL      |NULL        |\n",
      "|10187291046 |Ultricies Dignissim Lacus Foundation|Jewellery & Watches    |NULL      |NULL        |\n",
      "|10192359162 |Enim Condimentum PC                 |Culture & Entertainment|NULL      |NULL        |\n",
      "|10206519221 |Fusce Company                       |Gifts & Novelties      |NULL      |NULL        |\n",
      "|10255988167 |Aliquam Enim Incorporated           |Software & IT          |NULL      |NULL        |\n",
      "|10264435225 |Ipsum Primis Ltd                    |Jewellery & Watches    |NULL      |NULL        |\n",
      "|10279061213 |Pede Ultrices Industries            |Software & IT          |NULL      |NULL        |\n",
      "|10323485998 |Nunc Inc.                           |Home & Furniture       |NULL      |NULL        |\n",
      "|10342410215 |Facilisis Facilisis Corp.           |Software & IT          |NULL      |NULL        |\n",
      "|10346855916 |Odio Institute                      |Home & Furniture       |NULL      |NULL        |\n",
      "|10364012396 |Rutrum Justo Ltd                    |Culture & Entertainment|NULL      |NULL        |\n",
      "|10385011947 |Tellus Foundation                   |Crafts & Flowers       |NULL      |NULL        |\n",
      "|10385163239 |Sed Et Company                      |Crafts & Flowers       |NULL      |NULL        |\n",
      "|10385250025 |Id Ltd                              |Software & IT          |NULL      |NULL        |\n",
      "|10404542215 |Consequat Foundation                |Repair & Leasing       |NULL      |NULL        |\n",
      "|10430380319 |Sit Amet Nulla Corp.                |Automotive             |NULL      |NULL        |\n",
      "|10441711491 |Massa Vestibulum Foundation         |Automotive             |NULL      |NULL        |\n",
      "|10462560289 |Ut Consulting                       |Gifts & Novelties      |NULL      |NULL        |\n",
      "+------------+------------------------------------+-----------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "[Total number of merchants]: 4026\n"
     ]
    }
   ],
   "source": [
    "# Clean the fraud probability table\n",
    "mf = (\n",
    "    merchant_fraud\n",
    "    .withColumn(\"order_date\", F.to_date(\"order_datetime\"))\n",
    "    .withColumn(\"m_fraud_prob\", F.col(\"fraud_probability\").cast(\"double\"))\n",
    "    .drop(\"fraud_probability\")\n",
    ")\n",
    "\n",
    "# Clean the merchant meta table\n",
    "m_base = tbl_merchants_enriched.alias(\"m\")  \n",
    "\n",
    "m_name = (\n",
    "    tbl_merchants_enriched\n",
    "    .select(\n",
    "        F.col(\"merchant_abn\").alias(\"merchant_abn\"),\n",
    "        F.col(\"name\").alias(\"m_name\"),\n",
    "        F.col(\"tag_category_top\").alias(\"m_category\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# The left link ensures that all merchants retain it\n",
    "merchant_joined = (\n",
    "    m_base\n",
    "    .join(m_name, on=\"merchant_abn\", how=\"left\")   # Ensure that all merchants have meta information\n",
    "    .join(mf, on=\"merchant_abn\", how=\"left\")      # Then connect \"fraud probability\"\n",
    ")\n",
    "\n",
    "# Preview\n",
    "merchant_joined.select(\n",
    "    \"merchant_abn\", \"m_name\", \"m_category\", \"order_date\", \"m_fraud_prob\"\n",
    ").show(20, truncate=False)\n",
    "\n",
    "# Count the total number of merchants\n",
    "print(\"[Total number of merchants]:\", merchant_joined.select(\"merchant_abn\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd022956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[m_category NULL counts]: 0\n",
      "[The number of merchants with fraud probability ]: 48\n",
      "+------------+-----------------------+-------------------+------------------+\n",
      "|merchant_abn|m_name                 |m_category         |m_fraud_prob      |\n",
      "+------------+-----------------------+-------------------+------------------+\n",
      "|11149063370 |Et Arcu Limited        |Other              |51.01538421455241 |\n",
      "|11149063370 |Et Arcu Limited        |Other              |52.407803322764764|\n",
      "|11149063370 |Et Arcu Limited        |Other              |56.43761254995139 |\n",
      "|11470993597 |Sed Associates         |Jewellery & Watches|63.37734364737917 |\n",
      "|11590404675 |Arcu Sed PC            |Repair & Leasing   |29.607818240092094|\n",
      "|14530561097 |Duis At Inc.           |Jewellery & Watches|80.80054474543395 |\n",
      "|15043504837 |Odio Incorporated      |Jewellery & Watches|25.054391991473924|\n",
      "|15043504837 |Odio Incorporated      |Jewellery & Watches|26.12523097610844 |\n",
      "|15043504837 |Odio Incorporated      |Jewellery & Watches|59.77648897297805 |\n",
      "|15157368385 |Tempus Non Lacinia Inc.|Crafts & Flowers   |64.2774131928303  |\n",
      "+------------+-----------------------+-------------------+------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Check if there is NULL in m_category\n",
    "null_category_count = merchant_joined.filter(F.col(\"m_category\").isNull()).count()\n",
    "print(f\"[m_category NULL counts]: {null_category_count}\")\n",
    "\n",
    "# Statistics on merchants with fraud probability (de-duplication)\n",
    "fraud_merchants_count = merchant_joined.filter(F.col(\"m_fraud_prob\").isNotNull()) \\\n",
    "                                      .select(\"merchant_abn\").distinct().count()\n",
    "print(f\"[The number of merchants with fraud probability ]: {fraud_merchants_count}\")\n",
    "\n",
    "# If you want to preview merchants with fraud probability directly\n",
    "merchant_joined.filter(F.col(\"m_fraud_prob\").isNotNull()) \\\n",
    "               .select(\"merchant_abn\", \"m_name\", \"m_category\", \"m_fraud_prob\") \\\n",
    "               .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64f434f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The number of unique merchants with fraud_probability in merchant_fraud]: 61\n",
      "+------------+------+----------+----------+\n",
      "|merchant_abn|n_rows|first_date|last_date |\n",
      "+------------+------+----------+----------+\n",
      "|31334588839 |12    |2021-09-23|2021-12-26|\n",
      "|19492220327 |8     |2021-10-08|2022-01-29|\n",
      "|90918180829 |7     |2021-09-02|2022-02-19|\n",
      "|83199298021 |6     |2021-03-25|2022-02-27|\n",
      "|93260930990 |5     |2021-11-09|2021-12-14|\n",
      "|14827550074 |4     |2021-11-26|2021-12-12|\n",
      "|19010030815 |3     |2021-09-28|2021-12-24|\n",
      "|90568944804 |3     |2021-11-26|2021-11-29|\n",
      "|48534649627 |3     |2021-11-26|2021-11-29|\n",
      "|11149063370 |3     |2021-08-28|2022-02-25|\n",
      "+------------+------+----------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# How many unique merchants in the source table (or before joining) actually carry fraud_probability?\n",
    "mf_unique_merchants = merchant_fraud.filter(F.col(\"fraud_probability\").isNotNull()) \\\n",
    "                                    .select(\"merchant_abn\").distinct().count()\n",
    "print(f\"[The number of unique merchants with fraud_probability in merchant_fraud]: {mf_unique_merchants}\")\n",
    "\n",
    "# How many fraud records does each merchant have (see why the number of rows > the number of merchants)\n",
    "merchant_fraud.groupBy(\"merchant_abn\") \\\n",
    "    .agg(F.count(\"*\").alias(\"n_rows\"),\n",
    "         F.min(\"order_datetime\").alias(\"first_date\"),\n",
    "         F.max(\"order_datetime\").alias(\"last_date\")) \\\n",
    "    .orderBy(F.desc(\"n_rows\")) \\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb42897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of merchants that are present in the fraud file but missing in the merchant table: 13\n",
      "+------------+\n",
      "|merchant_abn|\n",
      "+------------+\n",
      "|14827550074 |\n",
      "|19010030815 |\n",
      "|23686790459 |\n",
      "|29674997261 |\n",
      "|57564805948 |\n",
      "|59258669983 |\n",
      "|73052515151 |\n",
      "|75892370170 |\n",
      "|81146325646 |\n",
      "|82999039227 |\n",
      "|83220249221 |\n",
      "|94311056026 |\n",
      "|99989036621 |\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the distinct merchant with fraud\n",
    "fraud_merchants = merchant_fraud.select(\"merchant_abn\").distinct()\n",
    "\n",
    "# Main table merchants\n",
    "all_merchants = tbl_merchants_enriched.select(\"merchant_abn\").distinct()\n",
    "\n",
    "# Find those that are in fraud but not in the merchant's main table\n",
    "missing_merchants = fraud_merchants.join(all_merchants, on=\"merchant_abn\", how=\"left_anti\")\n",
    "\n",
    "print(\"The number of merchants that are present in the fraud file but missing in the merchant table:\", missing_merchants.count())\n",
    "missing_merchants.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea7d9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only retain the columns that are ultimately needed\n",
    "merchant_joined = merchant_joined.select(\n",
    "    F.col(\"merchant_abn\"),\n",
    "    F.col(\"order_date\"),\n",
    "    F.col(\"m_fraud_prob\"),\n",
    "    F.col(\"m_name\"),\n",
    "    F.col(\"m_category\")\n",
    ")\n",
    "\n",
    "# Unify lowercase column names\n",
    "for col_name in merchant_joined.columns:\n",
    "    merchant_joined = merchant_joined.withColumnRenamed(col_name, col_name.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f04b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merchant profile saved to part1_data/merchant_profile.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/18 17:13:31 WARN TransportChannelHandler: Exception in connection from /10.5.82.157:59942\n",
      "java.io.IOException: Operation timed out\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.read0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:47)\n",
      "\tat java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:330)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:284)\n",
      "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:259)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:417)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:255)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:356)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:796)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:732)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:658)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:998)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/09/18 19:54:48 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 242391 ms exceeds timeout 120000 ms\n",
      "25/09/18 19:54:48 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "25/09/18 19:54:54 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 19:54:54 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 19:55:31 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 19:55:31 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:12:32 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:12:32 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:29:43 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:29:43 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:31:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:31:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:31:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/09/18 20:31:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/09/18 20:32:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:32:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:33:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:34:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:35:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:36:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:37:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:20 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:20 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:30 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:30 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:40 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:40 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:38:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:39:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:39:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:39:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:39:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:49:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 20:49:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 21:07:26 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 21:07:26 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 21:14:13 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 21:14:13 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 21:48:57 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 21:48:57 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:01:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:01:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:01:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:01:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:01:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:01:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:02:05 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:02:05 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:02:15 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:02:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:02:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:02:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.5.82.157:59941\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/09/18 22:02:25 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 59976)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Save merchant_joined as a parquet file for later analysis\n",
    "merchant_path = \"part1_data/merchant_profile.parquet\"\n",
    "merchant_joined.write.mode(\"overwrite\").parquet(merchant_path)\n",
    "print(f\"Merchant profile saved to {merchant_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
